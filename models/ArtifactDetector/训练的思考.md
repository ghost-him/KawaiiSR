训练的时候可以把dropout的值变成动态改变的。

比如这样：

```python
# 训练策略建议
class DropoutScheduler:
    def __init__(self, model, initial_dropout=0.4):
        self.model = model
        self.initial_dropout = initial_dropout
        self.stage_last = -1
    
    def adjust_dropout(self, epoch, total_epochs):
        stage = -1
        if epoch < total_epochs * 0.3:  # 前30%的训练
            dropout_rate = self.initial_dropout
            stage = 0
        elif epoch < total_epochs * 0.5:  # 30%-50%的训练
            dropout_rate = self.initial_dropout * 0.8
            stage = 1
        elif epoch < total_epochs * 0.7:
            dropout_rate = self.initial_dropout * 0.5
            stage = 2
        elif epoch < total_epochs * 0.9:
            dropout_rate = self.initial_dropout * 0.3
            stage = 3
        else:  # 最后10%的训练
            dropout_rate = 0.0
            stage = 4
        
        # 动态调整模型中的dropout
        if stage != self.stage_last:
            for module in self.model.modules():
                if isinstance(module, nn.Dropout):
                    module.p = dropout_rate
        stage_last = stage
```

这个模型可以将最后一层的特征图导出来。这样我可以在超分网络中，通过对比这个特征图来计算（比如使用mse来计算），但是这样就不知道效果怎么样了。到时候还得再看看。